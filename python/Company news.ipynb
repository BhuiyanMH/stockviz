{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill them with credentials\n",
    "ckey = \"\"\n",
    "skey = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = ckey\n",
    "consumer_secret = skey\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TECH SECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['text','date','link'])\n",
    "\n",
    "tweets = tweepy.Cursor(api.user_timeline,\n",
    "                   id = \"technews_today\",\n",
    "                   since=\"2019-09-03\",\n",
    "                   until=\"2021-01-01\"\n",
    "                   ).items()\n",
    "\n",
    "for t in tweets:\n",
    "    if \"2019-09-02\" in str(t.created_at):\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            df = df.append([{'text':t.text,'date':t.created_at,'link':t.entities['urls'][0]['expanded_url']}],ignore_index = True)\n",
    "        except:\n",
    "            df = df.append([{'text':t.text,'date':t.created_at,'link':\"\"}],ignore_index = True)\n",
    "\n",
    "df =df.loc[df['date'] <= '2021-01-01 00:00:00']\n",
    "\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/backup_data/tech_news.csv',sep = '\\t',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the news for every company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/backup_data/tech_news.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tesla = df.loc[df['text'].str.contains(\"tesla|musk|elon|spacex|neuralink|cybertruck\")].copy()\n",
    "df_tesla['Company'] = 'Tesla, Inc.'\n",
    "news_df = news_df.append(df_tesla)\n",
    "#df_tesla.to_csv(\"data/companies_news/\" + \"tesla\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_apple = df.loc[df['text'].str.contains(\"apple|black friday\")].copy()\n",
    "df_apple['Company'] = 'Apple Inc.'\n",
    "news_df = news_df.append(df_apple)\n",
    "#df_apple.to_csv(\"data/companies_news/\" + \"apple\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_microsoft = df.loc[df['text'].str.contains(\"gates|microsoft|cortana|surface|teams|outlook\")].copy()\n",
    "df_microsoft['Company'] = 'Microsoft Corporation'\n",
    "news_df = news_df.append(df_microsoft)\n",
    "#df_microsoft.to_csv(\"data/companies_news/\" + \"microsoft\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_amazon = df.loc[df['text'].str.contains(\"amazon|bezos|prime|echo|black friday\")].copy()\n",
    "df_amazon['Company'] = 'Amazon.com, Inc.'\n",
    "news_df = news_df.append(df_amazon)\n",
    "#df_amazon.to_csv(\"data/companies_news/\" + \"amazon\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_alibaba = df.loc[df['text'].str.contains(\"alibaba|black friday\")].copy()\n",
    "news_df = news_df.append(df_alibaba)\n",
    "#df_alibaba.to_csv(\"data/companies_news/\" + \"alibaba\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_google = df.loc[df['text'].str.contains(\"google|chrome|chromecast|chromebook|verily|youtube|stadia|android|alphabet\")].copy()\n",
    "df_google['Company'] = 'Alphabet Inc.'\n",
    "news_df = news_df.append(df_google)\n",
    "#df_google.to_csv(\"data/companies_news/\" + \"google\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_samsung = df.loc[df['text'].str.contains(\"samsung|galaxy\")].copy()\n",
    "df_samsung['Company'] = 'Samsung Electronics Co., Ltd.'\n",
    "news_df = news_df.append(df_samsung)\n",
    "#df_samsung.to_csv(\"data/companies_news/\" + \"samsung\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_nvidia = df.loc[df['text'].str.contains(\"nvidia|geforce|bethesda\")].copy()\n",
    "df_nvidia['Company'] = 'NVIDIA Corporation'\n",
    "news_df = news_df.append(df_nvidia)\n",
    "#df_nvidia.to_csv(\"data/companies_news/\" + \"nvidia\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_netflix = df.loc[df['text'].str.contains(\"netflix\")].copy()\n",
    "df_netflix['Company'] = 'Netflix, Inc.'\n",
    "news_df = news_df.append(df_netflix)\n",
    "#df_netflix.to_csv(\"data/companies_news/\" + \"netflix\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_facebook = df.loc[df['text'].str.contains(\"facebook|zuckerberg|instagram\")].copy()\n",
    "df_facebook['Company'] = 'Facebook, Inc.'\n",
    "news_df = news_df.append(df_facebook)\n",
    "#df_facebook.to_csv(\"data/companies_news/\" + \"facebook\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_sony = df.loc[df['text'].str.contains(\"sony|playstation|ps4|ps5\")].copy()\n",
    "news_df = news_df.append(df_sony)\n",
    "#df_sony.to_csv(\"data/companies_news/\" + \"sony\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_uber = df.loc[df['text'].str.contains(\"uber\")].copy()\n",
    "news_df = news_df.append(df_uber)\n",
    "#df_uber.to_csv(\"data/companies_news/\" + \"uber\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_activision = df.loc[df['text'].str.contains(\"activision|duty\")].copy()\n",
    "df_activision['Company'] = 'Activision Blizzard, Inc.'\n",
    "news_df = news_df.append(df_activision)\n",
    "#df_activision.to_csv(\"data/companies_news/\" + \"activision\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_adobe = df.loc[df['text'].str.contains(\"adobe\")].copy()\n",
    "df_adobe['Company'] = 'Adobe Inc.'\n",
    "news_df = news_df.append(df_adobe)\n",
    "#df_adobe.to_csv(\"data/companies_news/\" + \"adobe\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_spotify = df.loc[df['text'].str.contains(\"spotify\")].copy()\n",
    "news_df = news_df.append(df_spotify)\n",
    "#df_spotify.to_csv(\"data/companies_news/\" + \"spotify\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_huawei = df.loc[df['text'].str.contains(\"huawei\")].copy()\n",
    "news_df = news_df.append(df_huawei)\n",
    "#df_huawei.to_csv(\"data/companies_news/\" + \"huawei\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_nintendo = df.loc[df['text'].str.contains(\"nintendo|switch\")].copy()\n",
    "news_df = news_df.append(df_nintendo)\n",
    "#df_nintendo.to_csv(\"data/companies_news/\" + \"nintendo\" + \".csv\",index = False,sep = \"\\t\")\n",
    "\n",
    "df_xiaomi = df.loc[df['text'].str.contains(\"xiaomi\")].copy()\n",
    "df_xiaomi['Company'] = 'Xiaomi Corporation'\n",
    "news_df = news_df.append(df_xiaomi)\n",
    "#df_xiaomi.to_csv(\"data/companies_news/\" + \"xiaomi\" + \".csv\",index = False,sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['text','date','link'])\n",
    "\n",
    "tweets = tweepy.Cursor(api.user_timeline,\n",
    "                   id = \"WorldPharmaNews\",\n",
    "                   since=\"2019-09-03\",\n",
    "                   until=\"2021-01-01\"\n",
    "                   ).items()\n",
    "\n",
    "for t in tweets:\n",
    "    if \"2019-05\" in str(t.created_at):\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            df = df.append([{'text':t.text,'date':t.created_at,'link':t.entities['urls'][0]['expanded_url']}],ignore_index = True)\n",
    "        except:\n",
    "            df = df.append([{'text':t.text,'date':t.created_at,'link':\"\"}],ignore_index = True)\n",
    "\n",
    "df =df.loc[df['date'] <= '2021-01-01 00:00:00']\n",
    "\n",
    "df['text'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/backup_data/pharma_news.csv',sep='\\t',index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the news for the healthcare sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/backup_data/pharma_news.csv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfizer_df = df.loc[df['text'].str.contains('pfizer')].copy()\n",
    "pfizer_df['Company'] = 'Pfizer Inc.'\n",
    "news_df = news_df.append(pfizer_df)\n",
    "\n",
    "bayer_df = df.loc[df['text'].str.contains('bayer')].copy()\n",
    "bayer_df['Company'] = 'Bayer Aktiengesellschaft'\n",
    "news_df = news_df.append(bayer_df)\n",
    "\n",
    "sanofi_df = df.loc[df['text'].str.contains('sanofi')].copy()\n",
    "sanofi_df['Company'] = 'Sanofi'\n",
    "news_df = news_df.append(sanofi_df)\n",
    "\n",
    "abbott_df = df.loc[df['text'].str.contains('abbott')].copy()\n",
    "abbott_df['Company'] = 'Abbott Laboratories'\n",
    "news_df = news_df.append(abbott_df)\n",
    "\n",
    "lilly_df = df.loc[df['text'].str.contains('lilly')].copy()\n",
    "lilly_df['Company'] = 'Eli Lilly and Company'\n",
    "news_df = news_df.append(lilly_df)\n",
    "\n",
    "johnson_df = df.loc[df['text'].str.contains('johnson')].copy()\n",
    "johnson_df['Company'] = 'Johnson & Johnson'\n",
    "news_df = news_df.append(johnson_df)\n",
    "\n",
    "squibb_df = df.loc[df['text'].str.contains('squibb')].copy()\n",
    "squibb_df['Company'] = 'Bristol-Myers Squibb Company'\n",
    "news_df = news_df.append(squibb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = []\n",
    "for x in news_df['date']:\n",
    "    day = str(x).split(' ')[0]\n",
    "    days.append(day)\n",
    "news_df['date'] = days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(serie):\n",
    "    new_caps = []\n",
    "    for caption in serie:\n",
    "        new_caption = ''\n",
    "        for word in caption.split(' '):\n",
    "            if word[0:4] == 'http':\n",
    "                continue\n",
    "            else:\n",
    "                if new_caption == '':\n",
    "                    new_caption = word\n",
    "                else:\n",
    "                    new_caption = new_caption + ' ' + word\n",
    "        new_caps.append(new_caption)\n",
    "    return pd.Series(new_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = remove_html(news_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv('data/updated_news.csv',sep = '\\t',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing useless dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_csv('data/companies/news.csv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = pd.read_csv('data/companies/comp.csv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(df_stock['date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missed_dates = list(whole_dates.loc[~whole_dates['date'].isin(present_dates['date'])]['date'])\n",
    "df_news = df_news.loc[df_news['date'].isin(dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.to_csv('data/trimmed.csv',sep = '\\t',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('data/temp_news1.csv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which ones are the most named companies\n",
    "names = []\n",
    "for i,row in df.iterrows():\n",
    "    row_names = []\n",
    "    for word in row['text'].split():\n",
    "        if word[0].isupper():\n",
    "            row_names.append(word)\n",
    "    names.append(row_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_words = []\n",
    "for i,row in df.iterrows():\n",
    "    words = row['text'].split(' ')\n",
    "    for word in words:\n",
    "        if word[0].isupper():\n",
    "            if word not in cap_words:\n",
    "                cap_words.append(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
